# F1 å†³ç­–æ¨¡å— - å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ å¿«é€Ÿæµ‹è¯•æ–°çš„ F1 æ¨¡å—

### 1. è¿è¡Œå•å…ƒæµ‹è¯•

```bash
cd /Users/hefen/Desktop/husband/CECOFramework-main/vllm_llama_inference_framework
python3 tests/test_f1_core.py
```

**é¢„æœŸè¾“å‡º**:
```
âœ… 6 é€šè¿‡, âŒ 0 å¤±è´¥
ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼F1 æ¨¡å—æ ¸å¿ƒé€»è¾‘æ­£å¸¸å·¥ä½œ
```

---

### 2. æµ‹è¯•ä¸åŒåœºæ™¯çš„å†³ç­–

åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬ `test_scenarios.py`:

```python
from edge.f1_decision import F1_DecisionModule
from common.types import InferenceRequest, TaskRequirements

# åŠ è½½é…ç½®
import yaml
with open('config/config.yaml', 'r') as f:
    full_config = yaml.safe_load(f)

f1_config = full_config['edge']['f1']
f1 = F1_DecisionModule(f1_config)

# åœºæ™¯1: æ—¶æ•ä»»åŠ¡ï¼ˆèŠå¤©å¯¹è¯ï¼‰
print("\n=== åœºæ™¯1: èŠå¤©å¯¹è¯ ===")
request = InferenceRequest(
    prompt="Hi, how are you?",
    requirements=TaskRequirements(
        max_latency_ms=100,  # è¦æ±‚å¿«é€Ÿå“åº”
        priority=2
    )
)
plan = f1.decide(request)
print(f"å†³ç­–: {plan.strategy.value}")
print(f"ç†ç”±: {plan.reason}")

# åœºæ™¯2: é«˜è´¨é‡åˆ›ä½œ
print("\n=== åœºæ™¯2: æ–‡ç« åˆ›ä½œ ===")
request = InferenceRequest(
    prompt="Write a detailed article about climate change...",
    requirements=TaskRequirements(
        min_quality_score=0.95,  # è¦æ±‚é«˜è´¨é‡
        max_latency_ms=5000
    )
)
plan = f1.decide(request)
print(f"å†³ç­–: {plan.strategy.value}")
print(f"ç†ç”±: {plan.reason}")

# åœºæ™¯3: éšç§æ•æ„Ÿ
print("\n=== åœºæ™¯3: éšç§æ•°æ® ===")
request = InferenceRequest(
    prompt="My credit card is 1234...",
    requirements=TaskRequirements(
        privacy_level=2  # ç»å¯†çº§åˆ«
    )
)
plan = f1.decide(request)
print(f"å†³ç­–: {plan.strategy.value}")
print(f"ç†ç”±: {plan.reason}")

# åœºæ™¯4: å¹³è¡¡åœºæ™¯
print("\n=== åœºæ™¯4: å¸¸è§„é—®ç­” ===")
request = InferenceRequest(
    prompt="What is machine learning?",
    requirements=TaskRequirements(
        max_latency_ms=2000,
        min_quality_score=0.8
    )
)
plan = f1.decide(request)
print(f"å†³ç­–: {plan.strategy.value}")
print(f"å¾—åˆ†: {plan.score:.3f}")
print(f"ç†ç”±: {plan.reason}")
```

è¿è¡Œï¼š
```bash
python3 test_scenarios.py
```

---

### 3. æŸ¥çœ‹å†³ç­–æ—¥å¿—è¯¦æƒ…

F1 æ¨¡å—ä¼šè‡ªåŠ¨è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼Œæ˜¾ç¤ºï¼š
- ç³»ç»ŸçŠ¶æ€ï¼ˆCPUã€å†…å­˜ï¼‰
- ä»»åŠ¡éœ€æ±‚ï¼ˆSLOå»¶è¿Ÿã€è´¨é‡è¦æ±‚ã€ä¼˜å…ˆçº§ï¼‰
- å†³ç­–ç­–ç•¥å’Œç†ç”±
- æ‰§è¡Œå‚æ•°ï¼ˆdraft_max_tokensã€confidence_thresholdç­‰ï¼‰

---

### 4. è°ƒæ•´é…ç½®å‚æ•°

ç¼–è¾‘ `config/config.yaml`ï¼š

```yaml
edge:
  f1:
    # è°ƒæ•´ç¡¬çº¦æŸé˜ˆå€¼
    hard_constraints:
      cpu_overload: 90.0      # é™ä½ CPU é˜ˆå€¼ï¼Œæ›´å®¹æ˜“å¸è½½åˆ°äº‘ç«¯
      ultra_low_latency: 100  # æé«˜å»¶è¿Ÿé˜ˆå€¼
    
    # è°ƒæ•´è¯„åˆ†æƒé‡
    scoring_weights:
      latency: 0.5    # æ›´é‡è§†å»¶è¿Ÿ
      cost: 0.2       # é™ä½æˆæœ¬æƒé‡
      quality: 0.3
    
    # è°ƒæ•´å»¶è¿Ÿä¼°ç®—
    latency_estimates:
      edge_only_ms: 50         # æ ¹æ®å®é™…æµ‹è¯•è°ƒæ•´
      cloud_direct_ms: 150
      speculative_standard_ms: 70
```

---

### 5. é›†æˆåˆ°å®Œæ•´ç³»ç»Ÿ

#### å¯åŠ¨æœåŠ¡å™¨

```bash
# ç»ˆç«¯1: å¯åŠ¨äº‘ç«¯æœåŠ¡å™¨
python3 start_cloud.py --config config/config.yaml

# ç»ˆç«¯2: å¯åŠ¨è¾¹ç«¯æœåŠ¡å™¨ï¼ˆå·²é›†æˆ F1ï¼‰
python3 start_edge.py --config config/config.yaml

# ç»ˆç«¯3: å‘é€æ¨ç†è¯·æ±‚
python3 main.py --mode client --prompt "Hello, how are you?"
```

#### è§‚å¯Ÿ F1 å†³ç­–

è¾¹ç«¯æœåŠ¡å™¨æ—¥å¿—ä¼šæ˜¾ç¤ºï¼š
```
[F1] ä¸Šä¸‹æ–‡: CPU=45.0%, å†…å­˜=4000MB, SLOå»¶è¿Ÿ<5000ms, è´¨é‡>0.80, ä¼˜å…ˆçº§=1
[F1] å†³ç­–å®Œæˆ: speculative_standard (å¾—åˆ†=0.833)
[Edge] F1å†³ç­–: speculative_standard (å¾—åˆ†=0.833, ç†ç”±=Score: 0.833)
```

---

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜1: ImportError: No module named 'psutil'

**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install psutil
```

æˆ–è€…ä½¿ç”¨ä¸ä¾èµ– psutil çš„æµ‹è¯•ï¼š
```bash
python3 tests/test_f1_core.py
```

### é—®é¢˜2: F1 å†³ç­–æ€»æ˜¯é€‰æ‹©åŒä¸€ä¸ªç­–ç•¥

**æ£€æŸ¥**:
1. æŸ¥çœ‹é…ç½®æ–‡ä»¶çš„æƒé‡è®¾ç½®
2. æŸ¥çœ‹å»¶è¿Ÿä¼°ç®—æ˜¯å¦å‡†ç¡®
3. è¿è¡Œæµ‹è¯•æŸ¥çœ‹è¯„åˆ†è¯¦æƒ…ï¼š
   ```python
   scorer = MultiObjectiveScorer(config)
   scored = scorer.score_strategies(context)
   for s in scored:
       print(f"{s.strategy.value}: {s.score:.3f}")
   ```

### é—®é¢˜3: æƒ³ç¦ç”¨ F1 ä½¿ç”¨æ—§é€»è¾‘

**ä¸´æ—¶æ–¹æ¡ˆ**: åœ¨ `edge_server.py` çš„ `process_inference` ä¸­æ³¨é‡Šæ‰ F1 è°ƒç”¨ï¼š
```python
# execution_plan = self.f1_decision.decide(inference_request)
# ä½¿ç”¨å›ºå®šç­–ç•¥
from common.types import ExecutionStrategy, ExecutionPlan
execution_plan = ExecutionPlan(
    strategy=ExecutionStrategy.SPECULATIVE_STANDARD,
    params={'draft_max_tokens': 64, 'confidence_threshold': 0.8}
)
```

---

## ğŸ“Š æŸ¥çœ‹å†³ç­–ç»Ÿè®¡

åˆ›å»ºä¸€ä¸ªç»Ÿè®¡è„šæœ¬ `stats.py`:

```python
from edge.f1_decision import F1_DecisionModule
from common.types import InferenceRequest, TaskRequirements, ExecutionStrategy
import random

# åˆå§‹åŒ–
config = {...}
f1 = F1_DecisionModule(config)

# æ¨¡æ‹Ÿ100ä¸ªè¯·æ±‚
decisions = {s.value: 0 for s in ExecutionStrategy}

for _ in range(100):
    # éšæœºç”Ÿæˆä»»åŠ¡
    latency = random.choice([100, 500, 1000, 3000, 5000])
    quality = random.choice([0.6, 0.8, 0.9, 0.95])
    priority = random.choice([1, 2, 3])
    
    request = InferenceRequest(
        prompt="test",
        requirements=TaskRequirements(
            max_latency_ms=latency,
            min_quality_score=quality,
            priority=priority
        )
    )
    
    plan = f1.decide(request)
    decisions[plan.strategy.value] += 1

# è¾“å‡ºç»Ÿè®¡
print("å†³ç­–åˆ†å¸ƒ:")
for strategy, count in decisions.items():
    print(f"  {strategy}: {count}%")
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

- âœ… ç†Ÿæ‚‰ F1 å†³ç­–é€»è¾‘
- âœ… æ ¹æ®å®é™…æµ‹è¯•è°ƒæ•´é…ç½®å‚æ•°
- âœ… å‡†å¤‡é˜¶æ®µ2ï¼šç½‘ç»œæ„ŸçŸ¥åŠŸèƒ½

**æœ‰é—®é¢˜ï¼Ÿ** æŸ¥çœ‹ `F1_IMPLEMENTATION_SUMMARY.md` è·å–å®Œæ•´æ–‡æ¡£ã€‚
