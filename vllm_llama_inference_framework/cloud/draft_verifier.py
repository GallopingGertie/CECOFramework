"""
F2: äº‘ç«¯ Draft éªŒè¯å™¨ - é²æ£’åŒ¹é…ä¿®å¤ç‰ˆ
ä½¿ç”¨æœ€é•¿å…¬å…±å‰ç¼€ (LCP) ç®—æ³•ï¼Œå½»åº•è§£å†³ split(' ') å¸¦æ¥çš„é”™ä½é—®é¢˜
"""
import asyncio
import time
from typing import List, Tuple, Dict, Any, Optional

try:
    from vllm import LLM, SamplingParams
except ImportError:
    print("âŒ è­¦å‘Š: æœªæ‰¾åˆ° vLLM")
    LLM = Any 
    SamplingParams = Any

from common.types import VerifyRequest, VerifyResponse

class DraftVerifier:
    def __init__(self, model_path: str, acceptance_threshold: float = 0.8):
        self.model_path = model_path
        self.acceptance_threshold = acceptance_threshold
        self.model = self._load_model(model_path)
    
    def _load_model(self, model_path: str):
        print(f"[Cloud] åŠ è½½ vLLM æ¨¡å‹: {model_path} (TPè‡ªåŠ¨é€‚é…)")
        import torch
        gpu_count = torch.cuda.device_count()
        tp_size = 4 if gpu_count >= 4 else 1
        
        try:
            return LLM(
                model=model_path,
                tensor_parallel_size=tp_size,
                dtype="float16",
                trust_remote_code=True,
                gpu_memory_utilization=0.85,
                max_model_len=2048,
                enforce_eager=False
            )
        except Exception as e:
            print(f"âŒ vLLM åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

    async def verify_draft(self, request: VerifyRequest) -> VerifyResponse:
        """éªŒè¯ Draft (å­—ç¬¦ä¸²çº§ç²¾å‡†åŒ¹é…)"""
        start_time = time.time()
        
        full_prompt = request.prompt
        # 1. è¿˜åŸç«¯ä¾§ç”Ÿæˆçš„å®Œæ•´å­—ç¬¦ä¸²
        draft_text_raw = "".join(request.draft_tokens)
        
        # 2. è®©äº‘ç«¯ç”Ÿæˆæ ‡å‡†ç­”æ¡ˆ (Ground Truth)
        # é•¿åº¦åªè¦æ¯” draft ç¨å¾®é•¿ä¸€ç‚¹å³å¯ï¼Œç¡®ä¿èƒ½è¦†ç›–
        max_verify_len = len(request.draft_tokens) + 20
        
        cloud_generated_text = await self._generate_ground_truth(
            full_prompt, 
            max_tokens=max_verify_len
        )
        
        # 3. ğŸš€ æ ¸å¿ƒé€»è¾‘: æœ€é•¿å…¬å…±å‰ç¼€åŒ¹é… (Character-level LCP)
        match_len = 0
        min_len = min(len(draft_text_raw), len(cloud_generated_text))
        
        # é€å­—ç¬¦æ¯”å¯¹
        for i in range(min_len):
            if draft_text_raw[i] == cloud_generated_text[i]:
                match_len += 1
            else:
                break
        
        # 4. åˆ¤æ–­ç»“æœ
        # accepted_text æ˜¯ draft ä¸­åŒ¹é…æˆåŠŸçš„éƒ¨åˆ†
        accepted_text = draft_text_raw[:match_len]
        # rejected_text æ˜¯ draft ä¸­é”™è¯¯çš„éƒ¨åˆ†
        rejected_text = draft_text_raw[match_len:]
        
        # è®¡ç®— Token çº§çš„æ¥å—ç‡ (ä¼°ç®—)
        # æˆ‘ä»¬ç”¨å­—ç¬¦é•¿åº¦æ¯”ä¾‹æ¥ä¼°ç®—ï¼Œæˆ–è€…ç®€å•åœ°çœ‹ draft æ˜¯å¦è¢«å®Œå…¨æ¥å—
        is_fully_accepted = (match_len == len(draft_text_raw))
        
        # ç»Ÿè®¡ "è¢«ä¿®æ­£çš„ Token æ•°"
        # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯å­—ç¬¦çº§æ¯”å¯¹ã€‚
        # é€»è¾‘ï¼šå¦‚æœ draft é•¿åº¦æ˜¯ 100 å­—ç¬¦ï¼ŒåŒ¹é…äº† 80 å­—ç¬¦ï¼Œé‚£æˆ‘ä»¬å°±è®¤ä¸º 20% çš„ token é”™äº†ã€‚
        total_chars = len(draft_text_raw)
        if total_chars > 0:
            acceptance_rate = match_len / total_chars
        else:
            acceptance_rate = 1.0 # ç©ºè‰ç¨¿ç®—å…¨å¯¹
            
        print(f"[Cloud] éªŒè¯ç»“æœ: Drafté•¿={len(draft_text_raw)}, åŒ¹é…é•¿={match_len}, æ¥å—ç‡={acceptance_rate:.1%}")
        
        # 5. æ„é€ æœ€ç»ˆè¾“å‡º
        # æœ€ç»ˆæ–‡æœ¬ = Prompt + (åŒ¹é…çš„ Draft éƒ¨åˆ†) + (Cloud ç”Ÿæˆçš„å‰©ä½™éƒ¨åˆ†)
        # Cloud ç”Ÿæˆçš„å‰©ä½™éƒ¨åˆ† = cloud_generated_text[match_len:]
        correction = cloud_generated_text[match_len:]
        final_text = accepted_text + correction
        
        # ä¸ºäº†å…¼å®¹æ¥å£è¿”å› tokens åˆ—è¡¨ï¼Œæˆ‘ä»¬ç®€å•åˆ‡åˆ†ä¸€ä¸‹ (ä»…ç”¨äºæ˜¾ç¤º)
        # æ³¨æ„ï¼šè¿™é‡Œçš„ tokens å¹¶ä¸ä¸¥æ ¼å¯¹åº”æ¨¡å‹ tokenizerï¼Œä»…ä¾›å‰ç«¯æˆ–æ—¥å¿—æŸ¥çœ‹
        verified_tokens = [accepted_text, correction] 
        
        # ä¿®æ­£ä½ç½®ï¼šè¿™é‡Œä¸å†è¿”å›å…·ä½“çš„ token index åˆ—è¡¨ï¼Œå› ä¸ºå­—ç¬¦çº§æ— æ³•ç²¾ç¡®å¯¹åº” token index
        # åªè¦ acceptance_rate < 1.0ï¼Œå°±è¯´æ˜æœ«å°¾æœ‰ä¿®æ­£
        corrected_positions = [-1] if not is_fully_accepted else []

        latency = (time.time() - start_time) * 1000
        
        return VerifyResponse(
            verified_tokens=verified_tokens,
            verified_token_ids=[],
            accepted_count=match_len, # è¿™é‡Œå€Ÿç”¨å­—æ®µå­˜å­—ç¬¦æ•°
            total_count=total_chars,  # è¿™é‡Œå€Ÿç”¨å­—æ®µå­˜å­—ç¬¦æ•°
            acceptance_rate=acceptance_rate,
            corrected_positions=corrected_positions,
            final_text=full_prompt + final_text, # è¿”å›åŒ…å« prompt çš„å…¨é‡æ–‡æœ¬
            latency_ms=latency
        )
    
    async def _generate_ground_truth(self, prompt: str, max_tokens: int) -> str:
        """è°ƒç”¨ vLLM ç”Ÿæˆ"""
        sampling_params = SamplingParams(
            temperature=0.0, # éªŒè¯å¿…é¡»ç”¨è´ªå¿ƒ
            max_tokens=max_tokens
        )
        
        loop = asyncio.get_event_loop()
        output = await loop.run_in_executor(
            None, 
            lambda: self.model.generate([prompt], sampling_params)
        )
        return output[0].outputs[0].text