# åŸºäº vLLM + llama.cpp çš„äº‘è¾¹ç«¯æ¨ç†æ¡†æ¶

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ä¸€ä¸ªæ¨¡å—åŒ–çš„äº‘è¾¹ç«¯åˆ†å¸ƒå¼æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒç½®ä¿¡åº¦åˆ¤æ–­ã€Draft-Verify æœºåˆ¶å’Œ KV Cache ä¼˜åŒ–ã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

- **æ¨¡å—åŒ–è®¾è®¡**: F1-F4 å››ä¸ªæ ¸å¿ƒæ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºæ¶ˆèå®éªŒ
- **äº‘è¾¹ååŒ**: è¾¹ç«¯(llama.cpp)ç”Ÿæˆ Draftï¼Œäº‘ç«¯(vLLM)éªŒè¯ä¿®æ­£
- **ç½®ä¿¡åº¦åˆ¤æ–­**: å¤šç§ç­–ç•¥è¯„ä¼°ç”Ÿæˆè´¨é‡
- **KV Cache ä¼˜åŒ–**: è¾¹ç«¯å’Œäº‘ç«¯å„è‡ªä¼˜åŒ–çš„ç¼“å­˜ç­–ç•¥
- **HTTP é€šä¿¡**: ç®€å•çš„ RESTful API æ¥å£
- **æ¶ˆèå®éªŒ**: å†…ç½®å®Œæ•´çš„å®éªŒæ¡†æ¶

## ğŸ—ï¸ æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Edge (llama.cpp) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Cloud (vLLM)    â”‚
â”‚   - Draftç”Ÿæˆ      â”‚          â”‚   - DraftéªŒè¯    â”‚
â”‚   - ç½®ä¿¡åº¦åˆ¤æ–­     â”‚          â”‚   - ç»“æœä¿®æ­£     â”‚
â”‚   - KV Cache      â”‚          â”‚   - KV Cache    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ æ ¸å¿ƒæ¨¡å— (F1-F4)

### F1: ç½®ä¿¡åº¦åˆ¤æ–­é€»è¾‘ (edge/confidence.py)
åŸºäºæ¦‚ç‡åˆ†å¸ƒçš„ç½®ä¿¡åº¦è®¡ç®—ï¼Œæ”¯æŒå¤šç§ç­–ç•¥:
- **MAX_PROB**: æœ€å¤§æ¦‚ç‡ç­–ç•¥
- **ENTROPY**: ç†µå€¼ç­–ç•¥ (ç†µè¶Šä½ç½®ä¿¡åº¦è¶Šé«˜)  
- **TEMPERATURE**: æ¸©åº¦ç¼©æ”¾ç­–ç•¥
- **TOP_K_AGG**: Top-K èšåˆç­–ç•¥

### F2: Draft-Verify æœºåˆ¶ (edge/draft_generator.py, cloud/draft_verifier.py)
- **è¾¹ç«¯ç”Ÿæˆ**: ä½¿ç”¨è½»é‡çº§æ¨¡å‹å¿«é€Ÿç”Ÿæˆ Draft tokens
- **äº‘ç«¯éªŒè¯**: ä½¿ç”¨å¤§æ¨¡å‹éªŒè¯å¹¶ä¿®æ­£ Draft
- **æ¥å—ç‡ç»Ÿè®¡**: ç›‘æ§ Draft è´¨é‡

### F3: KV Cache ç®¡ç† (edge/kv_cache.py, cloud/kv_cache.py)
- **è¾¹ç«¯ç¼“å­˜**: LRU æ·˜æ±°ï¼Œå‰ç¼€åŒ¹é…
- **äº‘ç«¯ç¼“å­˜**: å—åˆ†é…ï¼Œåˆ†å¸ƒå¼åŒæ­¥
- **æ¶ˆèæ”¯æŒ**: å¯ç¦ç”¨ç‰¹å®šåŠŸèƒ½

### F4: HTTP é€šä¿¡ (common/http_client.py, common/http_server.py)
- **å¼‚æ­¥é€šä¿¡**: aiohttp å®ç°
- **è‡ªåŠ¨é‡è¯•**: å¯é…ç½®çš„é‡è¯•æœºåˆ¶
- **è¿æ¥æ± **: é«˜æ•ˆçš„è¿æ¥ç®¡ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install -r requirements.txt
```

### å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨è¾¹ç«¯æœåŠ¡å™¨
python start_edge.py --config config/config.yaml

# å¯åŠ¨äº‘ç«¯æœåŠ¡å™¨  
python start_cloud.py --config config/config.yaml
```

### è¿è¡Œæ¨ç†

```bash
# äº¤äº’æ¨¡å¼
python main.py --mode interactive

# å®¢æˆ·ç«¯æ¨¡å¼
python main.py --mode client --prompt "What is artificial intelligence?"
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬æ¨ç†

```python
import asyncio
from common.http_client import EdgeCloudHTTPClient

async def main():
    async with EdgeCloudHTTPClient() as client:
        result = await client.full_inference_pipeline(
            prompt="Explain quantum computing",
            max_tokens=256,
            use_draft_verify=True,
            use_confidence_check=True
        )
        
        print(f"ç»“æœ: {result.text}")
        print(f"æ€»å»¶è¿Ÿ: {result.total_latency_ms:.2f}ms")
        print(f"æ¥å—ç‡: {result.acceptance_rate:.2%}")

asyncio.run(main())
```

### æ¶ˆèå®éªŒ

```bash
# è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ
python ablation_experiments.py
```

æ”¯æŒçš„å®éªŒ:
- **baseline**: æ‰€æœ‰åŠŸèƒ½å¯ç”¨
- **no_confidence**: ç¦ç”¨ç½®ä¿¡åº¦åˆ¤æ–­
- **no_draft_verify**: ç¦ç”¨ Draft-Verify
- **no_kv_cache**: ç¦ç”¨ KV Cache
- **edge_only**: åªä½¿ç”¨è¾¹ç«¯
- **cloud_only**: åªä½¿ç”¨äº‘ç«¯

## âš™ï¸ é…ç½®

### åŸºæœ¬é…ç½® (config/config.yaml)

```yaml
edge:
  model:
    path: "models/llama-7b-q4.gguf"
  confidence:
    strategy: "max_prob"
    threshold: 0.8
  kv_cache:
    enabled: true
    max_size: 1000

cloud:
  model:
    path: "models/vllm-llama-13b"
  draft_verifier:
    acceptance_threshold: 0.8
  kv_cache:
    enabled: true
    max_blocks: 10000
```

### æ¶ˆèå®éªŒé…ç½®

```yaml
experiments:
  ablations:
    - name: "no_confidence"
      description: "ç¦ç”¨ç½®ä¿¡åº¦åˆ¤æ–­"
      config_overrides:
        inference:
          features:
            use_confidence_check: false
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### åŸºå‡†æ€§èƒ½

| æ¨¡å¼ | å¹³å‡å»¶è¿Ÿ | ååé‡ | æ¥å—ç‡ |
|------|---------|--------|--------|
| åŸºå‡† (å…¨åŠŸèƒ½) | ~50ms | ~20 req/s | 85% |
| ç¦ç”¨ç½®ä¿¡åº¦ | ~45ms | ~22 req/s | 80% |
| ç¦ç”¨ Draft-Verify | ~200ms | ~5 req/s | N/A |
| ç¦ç”¨ KV Cache | ~60ms | ~15 req/s | 85% |
| ä»…è¾¹ç«¯ | ~30ms | ~30 req/s | N/A |
| ä»…äº‘ç«¯ | ~250ms | ~4 req/s | N/A |

*æ³¨: å®é™…æ€§èƒ½å–å†³äºæ¨¡å‹å¤§å°å’Œç¡¬ä»¶é…ç½®*

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰ç½®ä¿¡åº¦ç­–ç•¥

```python
from edge.confidence import ConfidenceEnsemble
from common.types import ConfidenceStrategy

# é›†æˆå¤šç§ç­–ç•¥
ensemble = ConfidenceEnsemble([
    ConfidenceStrategy.MAX_PROB,
    ConfidenceStrategy.ENTROPY,
    ConfidenceStrategy.TOP_K_AGG
])

score, individual = ensemble.ensemble_confidence(
    token_probs,
    weights=[0.4, 0.3, 0.3]
)
```

### æ‰¹é‡æ¨ç†

```python
import asyncio
from common.http_client import EdgeCloudHTTPClient

async def batch_inference(prompts):
    async with EdgeCloudHTTPClient() as client:
        tasks = [
            client.full_inference_pipeline(prompt=prompt, max_tokens=128)
            for prompt in prompts
        ]
        return await asyncio.gather(*tasks)

# ä½¿ç”¨
results = asyncio.run(batch_inference(["What is AI?", "How does ML work?"]))
```

### å¥åº·æ£€æŸ¥

```bash
# æ£€æŸ¥è¾¹ç«¯
curl http://localhost:8080/health

# æ£€æŸ¥äº‘ç«¯
curl http://localhost:8081/health
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæ¨¡å—æµ‹è¯•

```bash
python test_framework.py
```

### æµ‹è¯•è¦†ç›–ç‡

```bash
pytest --cov=edge --cov=cloud --cov=common --cov-report=html
```

## ğŸ“ˆ ç›‘æ§

### æ€§èƒ½æŒ‡æ ‡

æ¡†æ¶å†…ç½®æ€§èƒ½ç›‘æ§:

```python
from common.http_client import HTTPClient

client = HTTPClient("http://localhost:8080")
stats = client.get_client_stats()

print(f"è¯·æ±‚æ€»æ•°: {stats['requests_sent']}")
print(f"å¹³å‡å»¶è¿Ÿ: {stats['avg_latency_ms']:.2f}ms")
print(f"é”™è¯¯æ•°: {stats['errors']}")
```

### ç¼“å­˜ç»Ÿè®¡

```bash
# è¾¹ç«¯ç¼“å­˜
curl http://localhost:8080/cache/stats

# äº‘ç«¯ç¼“å­˜
curl http://localhost:8081/cache/stats
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. è¿æ¥å¤±è´¥**
```
Error: Cannot connect to edge/cloud server
```
- âœ… æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨
- âœ… æ£€æŸ¥ç«¯å£å·æ˜¯å¦æ­£ç¡®
- âœ… æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

**2. æ¨¡å‹åŠ è½½å¤±è´¥**
```
Error: Model not found
```
- âœ… æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
- âœ… ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨
- âœ… æ£€æŸ¥æ¨¡å‹æ ¼å¼ (gguf for llama.cpp)

**3. å†…å­˜ä¸è¶³**
```
Error: Out of memory
```
- âœ… å‡å°‘æ‰¹å¤„ç†å¤§å°
- âœ… å‡å°‘ KV Cache å¤§å°
- âœ… ä½¿ç”¨æ›´å°çš„æ¨¡å‹

**4. è¶…æ—¶é”™è¯¯**
```
Error: Request timeout
```
- âœ… å¢åŠ è¶…æ—¶æ—¶é—´
- âœ… æ£€æŸ¥ç½‘ç»œè¿æ¥
- âœ… å‡å°‘ç”Ÿæˆé•¿åº¦

### å¯ç”¨è°ƒè¯•æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“š æ–‡æ¡£

- [ä½¿ç”¨æŒ‡å—](USAGE.md) - è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜
- [API æ–‡æ¡£](docs/API.md) - API æ¥å£æ–‡æ¡£
- [æ¶æ„è®¾è®¡](docs/ARCHITECTURE.md) - æ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®! è¯·é˜…è¯» [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

### å¼€å‘æµç¨‹

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½æ¨ç†å¼•æ“
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - CPU ä¼˜åŒ–çš„ LLM æ¨ç†
- [aiohttp](https://github.com/aio-libs/aiohttp) - å¼‚æ­¥ HTTP å®¢æˆ·ç«¯/æœåŠ¡å™¨

## ğŸ“ è”ç³»æ–¹å¼

- é¡¹ç›®ç»´æŠ¤è€…: [Your Name](mailto:your.email@example.com)
- é¡¹ç›®é“¾æ¥: [https://github.com/yourusername/vllm-llama-inference-framework](https://github.com/yourusername/vllm-llama-inference-framework)

## ğŸ—ºï¸ è·¯çº¿å›¾

- [ ] æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹
- [ ] æ·»åŠ  WebSocket é€šä¿¡
- [ ] å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡
- [ ] é›†æˆæ›´å¤šæ¨ç†å¼•æ“
- [ ] æ·»åŠ å¯è§†åŒ–ç›‘æ§é¢æ¿
- [ ] æ”¯æŒæµå¼è¾“å‡º

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Star!**
